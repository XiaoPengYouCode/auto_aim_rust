# Change Log

## 2025.6.13下午 - 2025.6.13凌晨

1. 大致实现了 `rbt_log` 文件输出系统，可以根据配置来选择是否在命令行和文件中输出，并根据当前时间自动创建和归类文件夹，创建文件并写入日志信息（学到了很多 `tracing` 相关的知识）
2. 将 `rbt_cfg` 进行了重构，将配置文件进行分层，每个模块拥有自己的配置信息，同时也有一个 `general` 配置负责一些公共配置（这部分怎么写实话还没想好，先按照 Gemini 的建议写了，后面看使用下来有没有什么槽点）
3. 下次请优先扩展多帧运的能力

## 2025.6.14晚上 - 2025.6.15凌晨

1. 实现了一个概念版的 `rbt_benchmark` 用来记录 latency 和 fps，但是现在写完感觉好蠢，因为用了 `Mutex<HashMap>` 做实现

记录一下今天的 prompt，考虑考完模电之后把这个更高效地实现一下

> 请帮我审查我的代码，指出不够合理的地方，不要生成代码，而是用文字进行描述
>
> **需求**：用Rust实现CV流水线监控模块（目标150fps），检测延迟和FPS。流水线含三个顺序线程：
>
> - 取图+前处理（CPU，~20ms）
> - 推理（GPU，~30ms）
> - 后处理+输出（CPU，~20ms）
>
> **延迟检测**：
>
> - 线程1记录帧开始时间（start_time）
> - 线程3计算延迟（当前时间 - start_time）后丢弃帧数据
> - 使用process_id_counter跟踪处理进度
>
> **FPS统计**：
>
> - 1秒周期的轻量级tokio任务计算FPS
> - 统计后重置计数器
>
> **内存管理**：
>
> 可选方案：
>
> - 1秒周期清理未完成帧的时间戳
> - 或在Drop时检查残留数据
>
> 以及一些疑问，虽然我这里用到了三个线程，但是其实每一帧都是顺序执行的，我使用多线程的唯一目的是使用单槽口缓冲区来提高吞吐量（因为第二步要用到GPU，我可以在运行2的同时运行下一步的1或者上一步的3），那么是否使用环形缓冲区等数据结构要相比使用HaspMap加锁，来得更高

## 6.15 下午 6:00 开始 - 6.17下午四点结束

实现了基于 watch 的多线程, 简单记录了一下 FPS 和 lantency，结果如下

- lantency: 40
- FPS: 60

具体实现

使用了 tokio 异步运行时，一共三个运行时，pre, infer和 post，然后一起 join，每个运行时中，所有操作都放在了 blocking 块中（背后应该是线程池），避免阻塞主异步任务，符合 tokio 的设计模式，后续打算一直从这个方向走下去

另外我发现统计 FPS 和 lantency 没有我之前想象中的那么难，之前的工作可能要推倒重来了，我也不确定，再观望使用一段时间

期望下次打开之后，先完成摄像头取帧的工作，用 USB 摄像头来完成任务，而不是一直从硬盘读

## 6.21 以来

前几天一直发现基于Watch的方案可能不是很靠谱，因为需要clone，但我更需要的可能是所有权转移，最后兜兜转转到了 crossbeam-ArrayQueue，并且给帧加上了生命周期检测，可以根据自己的状态生成对应的log，可以体现出生产者速度过快，消费者丢失帧的情况

根据社区的反馈和AI的帮助，在量化模型的前后增加了两个数据转换的节点，这样我就不需要自己处理half了，输入f32之后GPU自动cast为f16，这真是太棒了，感谢社区，感谢 onnx 如此好的生态，再见Half

另外我突发奇想，其实归一化操作也可以放在 onnx 模型中来完成，所以我就把归一化操作给放进去了，这样我的代码又变简单了，很有收获的一天

## 6.22下午-6.23

1. 使用 lazy_static 统一config变量，加入 RwLock 便于之后扩展config热重载
2. 把 TensorRT 暂时移除了
